---
title: "Local Dependence in Exponential Random Graph Models"
author: "Nick Solomon"
date: "May 4, 2016"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["slides.css"]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      self_contained: true
---
class: center, middle, inverse

```{r setup, include=FALSE}
library(tidyverse)
library(ggraph)
library(broom)
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

## Random graphs

---

## What is a graph?

```{r out.width="25%", fig.align = "center"}
knitr::include_graphics("../writing/figure/png/small_net.png")
```

--

- A pair of edges and vertices, $G = (E, V)$.
--

  - Edges are pairs of vertices: $e = (i, j)$ for $i, j \in V$
--

- Can be represented as an adjacency matrix:
\begin{bmatrix}
0 & 1 & 0 & 1 \\\
1 & 0 & 0 & 1 \\\
0 & 0 & 0 & 1 \\\
1 & 1 & 1 & 0
\end{bmatrix}
---

## Random graphs
--

- Consider the graph as an adjacency matrix where each entry is a random variable
--

- Add vertex attributes
---
class: center, middle, inverse

## Exponential random graph models (ERGMs)

---
## ERGMs

- A broad class of flexible models for random graphs.
--

- Model network features selected by the researcher
--

- Has the form

$$
P(Y = y) = \frac{e^{g(y) \cdot \theta}}{C(\mathcal{Y}, \theta)}
$$

.center[where]

$$
C(\mathcal{Y}, \theta) = \sum_{y \in \mathcal{Y}} e^{g(y) \cdot \theta}.
$$

---

## Estimating $\theta$

--

- Difficult because of intractable normalizing constant 
--

- Maximum pseudolikelihood estimation (MPLE)
--

  - Simplifying assumption of dyad independence
--

  - Reduces the problem to logistic regression
--

- Markov chain Monte Carlo maximum likelihood estimation (MCMC-MLE)
--

  - Approximates a log ratio of normalizing constants
--

  - Generate a Markov chain converging to the distribution implied by $\theta_0$
  (typically the MPLE)
--

- The exchange algorithm
--

  - Augment the posterior with auxiliary variables to remove intractable constants
---

## Examples

```{r fig.align='center'}
library(statnet)
data(florentine)
ggraph(flomarriage) + 
  geom_edge_fan(edge_colour = "gray") +
  geom_node_point(aes(size = wealth)) +
  geom_node_text(aes(label = name), vjust = -1) +
  scale_y_continuous(expand = c(.1, 0))+
  theme_graph() +
  labs(title = "Florentine Marriage Network")
```

---

## MCMC-MLE Example

```{r include = FALSE, cache = TRUE}
flo_model <- ergm(flomarriage ~ edges() + 
                    triangle() + 
                    absdiff("wealth"))

flo_model_bayes <- Bergm::bergm(flomarriage ~ edges() + 
                    triangle() + 
                    absdiff("wealth"))
```

```{r echo = TRUE, cache = TRUE, eval = FALSE}
flo_model <- ergm::ergm(flomarriage ~ edges() + 
                    triangle() + 
                    absdiff("wealth"))
```
--

```{r}
broom::tidy(flo_model)
```
---
## Bayesian Example

```{r eval = FALSE, echo = TRUE}
flo_model_bayes <- Bergm::bergm(flomarriage ~ edges() + 
                    triangle() + 
                    absdiff("wealth"))
```
--

```{r, fig.asp = 1/2, fig.align = "center"}
flo_bayes_tidy <- data.frame(tidy(flo_model)$term, apply(flo_model_bayes$Theta, 2, mean), apply(flo_model_bayes$Theta, 2, sd))
names(flo_bayes_tidy) <- c("term", "posterior mean", "posterior s.d.")
flo_bayes_mcmc <- as.data.frame(apply(flo_model_bayes$Theta, 2, cbind))
names(flo_bayes_mcmc) <- tidy(flo_model)$term
long <- gather(flo_bayes_mcmc, variable, value)
flo_dens_plot <- ggplot(long, aes(value)) +
  geom_density()+
  facet_wrap(~variable, scales = "free")+
  theme_bw()
flo_bayes_tidy
flo_dens_plot
```

---
class: center, middle, inverse

## Random Networks
---

## Exponential random network models (ERNMs)
--

- A useful variation on random graphs with fixed vertex attributes
--

- Vertex attributes are random variables
--

- Let $Y$ be the graph and $X$ the matrix of vertex attributes
--

- A similar exponential form
$$
P(Y = y, X \in x| \eta) \propto e^{\eta \cdot g(y, x)}
$$
--

- With a similarly nasty normalizing constant
---
class: middle, center, inverse

## The good stuff
---

## Two theorems
--

1. Locally dependent sparse exponential random network models have strongly
consistent maximum likelihood estimates
--

2. Locally dependent sparse exponential random network models have asymptotically
normal statistics
--
 (most of the time)

---
## Local dependence

- A random network model satisfies the *local dependence property* if there is a partition of the node set $\mathcal{N}$ into neighborhoods $A_1, A_2, \dots, A_K$ for $K \geq 2$ such that the network variables $Z_{ij}$ are dependent when $i, j \in A_{\ell}$ for some $\ell$ and independent otherwise. We also require that nodal attributes depend only on the attributes of nodes within the same neighborhood.
--

- Then we can write the probability measure as
$$P(Z \in \mathbf{Z}) = \prod_{k = 1}^{K}\left[ P_{kk}(Z_{kk} \in \mathbf{Z}_{kk}) \prod_{\ell = 1}^{k-1} P_{kl}(Z_{kl} \in \mathbf{Z}_{kl}, Z_{lk} \in \mathbf{Z}_{lk}) \right],$$
where $Z_{mn}$ is the subnetwork consisting of the random graph ties from nodes in $A_m$ to those in $A_n$ and the appropriate node variables and $\mathbf{Z}_{mn}$ is a subset of the sample space of $Z_{mn}$
--

- The measures $P_{kk}$ can induce dependence between dyads while the measures $P_{kl}$ induce independence
---

## Local dependence (cont'd)
--

- Only your friends matter
---

## Sparsity

- There is some $\delta > 0$ and some $C > 0$ such that
$$\mathbb{E}\left( \left| Y_{Bij} \right|^{p} \right) \leq Cn^{-\delta}, \qquad (p = 1, 2)$$
where $n = |\mathcal{N}|$ and $Y_{Bij}$ signifies the tie between neighborhoods from node $i \in A_{l}$ to node $j \in A_{m}$ where $l \neq m$
--

- Most of your friends are in your neighborhood
---

## Existence and consistency of MLEs
--

- We can recover the distribution of a subnetwork from the larger network
--

- Therefore, we have a projective exponential family, and the result follows<sup>1</sup>

.footnote[1. Shalizi & Rinaldo, 2013]
---

## Asymptotic normality of statistics
--

- Most statistics are in one of three classes:
--

  1. Depend only on the graph, $Y$
--

  2. Depend on both the graph and the vertex attributes
--

  3. Depend only on the vertex attributes, $X$
--

- There are further technical conditions on the form of the statistics
---

## Classes 1 & 2
--

- Split the statistic into within and between neighborhood portions
--

- Bound the variance of the between neighborhood portion
--
  
  - Becomes asymptotically negligible
--

- Bound the variance of the within neighborhood portion
--

- Show Lyapounov's condition holds
--

- The result follow from the Lindeberg-Feller CLT and Slutsky's Theorem
---

## Class 3
--

- The vertex attributes form an $M$-dependent sequence of random variables
--

- The result follows from a central limit theorem for dependent variables<sup>1</sup>

.footnote[1. Billingsley, 1995]
---
class: center, middle, inverse

## Simulations
---

## A simulated network

```{Rcpp}
#include <Rcpp.h>

using namespace Rcpp;

//[[Rcpp::export]]
IntegerMatrix make_adj(List vert_df){
  NumericVector vert = vert_df["vert"];
  NumericVector nbhd = vert_df["nbhd"];
  NumericVector grp = vert_df["grp"];
  double n = vert.size();
  
  IntegerMatrix adj_mat(n,n);
  
  for(int i = 0; i < n; i++){
    for(int j = 0; j < n; j++){
      if(nbhd[i] == nbhd[j]){
        if(grp[i] == grp[j]){
          adj_mat(i,j) = as<int>(rbinom(1,1,.3));
          continue;
        } else {
          adj_mat(i,j) = as<int>(rbinom(1, 1, .1));
          continue;
          }
      } else{       
        adj_mat(i,j) = as<int>(rbinom(1, 1, 50/(n*n)));
        continue;
      }
    }
  }

  for(int i = 1; i < n/10; i++){
    for(int j = 0; j < n; j++){
      for(int k = 0; k < n; k++){
        for(int l = 0; l < n; l++){
          if(nbhd[j] == i &&
             nbhd[k] == i &&
             nbhd[l] == i){
            if(adj_mat(j, k) == 1 &&
               adj_mat(k, l) == 1 &&
               adj_mat(j, l) == 0){
              adj_mat(j, l) = as<int>(rbinom(1, 1, .75));
            }
          }
        }
      }
    }
  }

  return adj_mat;
}

//[[Rcpp::export]]
IntegerVector make_attr(List vert_df, IntegerMatrix adj_mat){
  IntegerVector attr = vert_df["attr"];
  IntegerVector nbhd = vert_df["nbhd"];
  int n = attr.size();
  for(int i = 0; i < n; i++){
    for(int j = 0; j < n; j++){
      if(attr[i] == 1 && adj_mat(i,j) == 1 && nbhd[i] == nbhd[j]){
        attr[j] = as<int>(rbinom(1, 1, .75));
      }else if(adj_mat(i,j) == 1 && nbhd[i] == nbhd[j]){
        attr[j] = as<int>(rbinom(1, 1, .1));
      }
    }
  }
  return attr;
}
```

```{r}
make_net <- function(n){
  vert_df <- tibble(vert = 1:n)
  nbhds <- 1:floor(n/10)
  vert_df <- vert_df %>% 
    mutate(nbhd = sample(nbhds, n, replace = TRUE),
           grp = rbinom(n, 1, .5),
           attr = rbinom(n, 1, .3))
  
  adj_mat <- make_adj(vert_df)
  
  vert_df$attr <- make_attr(vert_df, adj_mat)
  
  net <- network(adj_mat)
  net %v% "group" <- vert_df$grp
  net %v% "attribute" <- vert_df$attr
  net %v% "nbhd" <- vert_df$nbhd
  
  return(net)
}

node_match <- function(net){
  edge_list <- as.edgelist(net)
  grps <- net %v% "group"
  num <- 0
  for(i in 1:nrow(edge_list)){
    if(grps[edge_list[i, 1]] == grps[edge_list[i,2]]) num <- num + 1
  }
  num/nrow(edge_list)
}
```

```{r fig.align="center", cache=TRUE}
example_net <- make_net(100)
ggraph(example_net) +
  geom_edge_fan(end_cap = circle(1.5, "mm"),
                start_cap = circle(1.5, "mm")) +
  geom_node_point(aes(shape = as.factor(attribute),
                      color = as.factor(nbhd)),
                  size = 2.5) +
  scale_shape_manual(values = c("0" = 21, "1" = 19),
                     labels = c("0" = "Absent",
                                "1" = "Present"),
                     name = "Attribute")+
  guides(color = "none")+
  theme_graph()
```
---

## Many simulated networks

```{r cache = TRUE}
n_reps <- 500
results <- tibble(num = c(rep(20, n_reps),
                        rep(50, n_reps),
                        rep(100, n_reps),
                        rep(200, n_reps)))
results <- results %>% 
  mutate(net = map(num, make_net),
         mean_deg = map_dbl(net, ~ mean(degree(.x))),
         mean_attr = map_dbl(net, ~ mean(.x %v% "attribute")),
         prop_homo = map_dbl(net, node_match))

results <- results %>% 
  gather(stat, value, -(num:net))
```

```{r fig.asp= 1/2, fig.align="center", cache = TRUE}
facet_labs <- c("mean_attr" = "% Attribute",
                "mean_deg" = "Mean degree",
                "prop_homo" = "% Hom. ties",
                "20" = "\\(n\\) = 20",
                "50" = "\\(n\\) = 50",
                "100" = "\\(n\\) = 100",
                "200" = "\\(n\\) = 200",
                "500" = "\\(n\\) = 500")

ggplot(results, aes(value)) +
  geom_density(aes(col = as.factor(num)), size = 1) +
  facet_wrap(~stat, scales = "free", labeller = as_labeller(facet_labs)) +
  labs(x = "Value",
       y = "Density",
       col = "\\(n\\)") +
  theme_bw()
```
---

## Many simulated networks

```{r fig.align= "center", cache = TRUE}
ggplot(results, aes(sample = value)) +
  geom_qq(alpha = .5, size = .5) +
  geom_qq_line()+
  facet_grid(stat ~ num, scales = "free_y",  labeller = as_labeller(facet_labs)) +
  labs(x = "Theoretical quantiles",
       y = "Sample quantiles") +
  theme_bw()
```

---

## Finding standard errors
--

- Jackknife at the neighborhood level
--

```{r cache = TRUE}
n_vert <- 200
net <- make_net(n_vert)

rm_vertex <- function(net, i){
  adj_mat <- as.matrix.network.adjacency(net)
  vert_df <- tibble(grp = net %v% "group",
                    attr = net %v% "attribute")
  
  new_adj_mat <- adj_mat[-i, -i]
  new_vert_df <- vert_df[-i,]
  new_net <- network(new_adj_mat)
  new_net %v% "group" <- new_vert_df$grp
  new_net %v% "attribute" <- new_vert_df$attr
  new_net
}

sim_df <- results %>% 
  filter(num == n_vert) %>% 
  select(-num, -net) %>% 
  mutate(origin = "Simulation")

se_sim <- sim_df %>% 
  group_by(stat) %>% 
  summarise(se_simulation = sd(value))

nbhds <- net %v% "nbhd"
num_nbhds <- length(unique(nbhds))
boot3 <- numeric(num_nbhds)

jack_df <- tibble(mean_deg = numeric(num_nbhds),
                  mean_attr = numeric(num_nbhds),
                  prop_homo = numeric(num_nbhds))

for(i in 1:num_nbhds){
  temp_net <- rm_vertex(net, which(nbhds == i))
  jack_df$mean_deg[i] <- mean(degree(temp_net))
  jack_df$mean_attr[i] <- mean(temp_net %v% "attribute")
  jack_df$prop_homo[i] <- node_match(temp_net)
}

jack_df <- jack_df %>% 
  gather(stat, value) %>% 
  group_by(stat) %>% 
  summarise(jack_sd = sqrt((num_nbhds - 1)/num_nbhds * 
                             sum((value - mean(value))^2)))
```

.center[
```{r asis = TRUE}
se_out <- bind_cols(se_sim, jack_df)[,-3] %>% 
  mutate(stat = c("% Attribute",
                  "Mean degree",
                  "% Hom. ties"))
knitr::kable(se_out, format = "markdown", col.names = c("Statistic",
                                   "Simulated standard error",
                                   "Jackknife standard error"))
```
]

---
## Further research
--

- Software for fitting models
--

- Standard error estimates
--

- Multivariate extension of CLT for $M$-dependent random variables
